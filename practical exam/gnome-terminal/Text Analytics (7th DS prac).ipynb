{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194708df-50dc-4144-805f-cc513b7da53b",
   "metadata": {},
   "source": [
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of documents by calculating Term Frequency and Inverse\n",
    "DocumentFrequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104ca8a-4fd1-40fb-b73f-6cc38727dae0",
   "metadata": {},
   "source": [
    "## 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510c2f3f-4d2a-4f5d-bfc4-a212a7480a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ecb406-2618-4c48-8975-6b2d67e2c2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample document\n",
    "document = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "print(\"Tokenization:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d4707ab-4f66-4d50-8d62-b840c5f3cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tagging:\n",
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tagging:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d59e87e-36a8-4742-bf75-04a4a25f7bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopwords Removal:\n",
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords removal\n",
    "stop_words = stopwords.words('english')\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"\\nStopwords Removal:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c37800-9b89-42a8-a31c-776367638e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming:\n",
      "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8beb6d-7b65-41ab-aaf8-5d0d25878f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ae61611-7bc5-4cfd-89cc-286856178d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "[[0.         0.35272845 0.         0.25096919 0.35272845 0.\n",
      "  0.         0.35272845 0.25096919 0.         0.35272845 0.35272845\n",
      "  0.         0.50193839 0.         0.        ]\n",
      " [0.32412345 0.         0.32412345 0.2306165  0.         0.32412345\n",
      "  0.32412345 0.         0.2306165  0.32412345 0.         0.\n",
      "  0.32412345 0.2306165  0.32412345 0.32412345]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "\"The dog is very lazy. He likes to sleep all day.\"]\n",
    "\n",
    "# Calculate TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print TF-IDF matrix\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0458a7c-aa4b-4d18-9afb-378c58b920f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc 1:\n",
      "The 0.5019383882729199\n",
      "quick 0.3527284455688186\n",
      "brown 0.3527284455688186\n",
      "fox 0.3527284455688186\n",
      "jumps 0.3527284455688186\n",
      "over 0.3527284455688186\n",
      "the 0.5019383882729199\n",
      "lazy 0.25096919413645996\n",
      "dog 0.25096919413645996\n",
      "\n",
      "Doc 2:\n",
      "The 0.23061650387901597\n",
      "dog 0.23061650387901597\n",
      "is 0.3241234495558481\n",
      "very 0.3241234495558481\n",
      "lazy 0.23061650387901597\n",
      "He 0.3241234495558481\n",
      "likes 0.3241234495558481\n",
      "to 0.3241234495558481\n",
      "sleep 0.3241234495558481\n",
      "all 0.3241234495558481\n",
      "day 0.3241234495558481\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(documents):\n",
    "    print(f'\\nDoc {i+1}:')\n",
    "    for word in word_tokenize(doc):\n",
    "        if word!='.':\n",
    "            idx = tfidf_vectorizer.vocabulary_[word.lower()]\n",
    "            print(word, tfidf_matrix.toarray()[i][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6426f85f-8fcb-4fdf-9e1b-e24b5ecd6b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "34cea3bc-1330-407e-9194-358553af423c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in doc.split()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
